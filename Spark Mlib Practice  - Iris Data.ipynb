{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "from pyspark.ml import linalg\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "from pyspark.ml import feature\n",
    "\n",
    "from pyspark.ml import regression\n",
    "from pyspark.ml import classification\n",
    "from pyspark.ml import clustering\n",
    "\n",
    "from pyspark.ml import evaluation\n",
    "from pyspark.ml import param\n",
    "from pyspark.ml import util\n",
    "from pyspark.ml import tuning\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basicm statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:480"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create RDD of Vectors\n",
    "sample = sc.parallelize([np.array([1.0, 10.0, 100.0]), np.array([2.0, 20.0, 200.0]), np.array([3.0, 30.0, 300.0])])\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "colStats() returns an instance of MultivariateStatisticalSummary, which contains the column-wise max, min, mean, variance, and number of nonzeros, as well as the total count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.mllib.stat._statistics.MultivariateStatisticalSummary at 0x112611588>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statics = Statistics.colStats(sample)\n",
    "statics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2.   20.  200.]\n",
      "[  1.00000000e+00   1.00000000e+02   1.00000000e+04]\n",
      "[ 3.  3.  3.]\n"
     ]
    }
   ],
   "source": [
    "# a dense vector containing the mean value for each column\n",
    "print(statics.mean())\n",
    "\n",
    "# column-wise variance\n",
    "print(statics.variance())\n",
    "\n",
    "# number of nonzeros in each column\n",
    "print(statics.numNonzeros())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1.   10.  100.]\n"
     ]
    }
   ],
   "source": [
    "print(statics.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(statics.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "\n",
    "Compute the correlation (matrix) for the input RDD(s) using the specified method. Methods currently supported: pearson (default), spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation is: 0.8500286768773007\n"
     ]
    }
   ],
   "source": [
    "seriesX = sc.parallelize([1.0, 2.0, 3.0, 3.0, 5.0])\n",
    "seriesY = sc.parallelize([11.0, 22.0, 33.0, 33.0, 555.0])\n",
    "print(\"Correlation is: \" + str(Statistics.corr(seriesX, seriesY, method=\"pearson\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vectors\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "rdd = sc.parallelize([Vectors.dense([1, 0, 0, -2]), Vectors.dense([4, 5, 0, 3]),Vectors.dense([6, 7, 0,  8]), Vectors.dense([9, 0, 0, 1])])\n",
    "pearsonCorr = Statistics.corr(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.05564149         nan  0.40047142]\n",
      " [ 0.05564149  1.                 nan  0.91359586]\n",
      " [        nan         nan  1.                 nan]\n",
      " [ 0.40047142  0.91359586         nan  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(str(pearsonCorr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Iris Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define data struct\n",
    "struct = StructType([\n",
    "        StructField('Id', IntegerType(), True),\n",
    "        StructField('SepalLengthCm', DoubleType(), True),\n",
    "        StructField('SepalWidthCm', DoubleType(), True),\n",
    "        StructField('PetalLengthCm', DoubleType(), True),\n",
    "        StructField('PetalWidthCm', DoubleType(), True),\n",
    "        StructField('Species', StringType(), True)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, Row \n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "df_iris = sqlContext.read.load('../iris.csv',\n",
    "                               format='com.databricks.spark.csv', \n",
    "                               header='true', \n",
    "                               schema= struct)\n",
    "\n",
    "\n",
    "# Or load data from hdfs cluster\n",
    "# df_iris =spark.read.csv('hdfs:///user/aadsyanw/data/iris.csv', header=True, schema=struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- SepalLengthCm: double (nullable = true)\n",
      " |-- SepalWidthCm: double (nullable = true)\n",
      " |-- PetalLengthCm: double (nullable = true)\n",
      " |-- PetalWidthCm: double (nullable = true)\n",
      " |-- Species: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+-------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|Species|\n",
      "+---+-------------+------------+-------------+------------+-------+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2| Setosa|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2| Setosa|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2| Setosa|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2| Setosa|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2| Setosa|\n",
      "+---+-------------+------------+-------------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# top 5\n",
    "df_iris.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-------------------+------------------+------------------+---------+\n",
      "|summary|                Id|     SepalLengthCm|       SepalWidthCm|     PetalLengthCm|      PetalWidthCm|  Species|\n",
      "+-------+------------------+------------------+-------------------+------------------+------------------+---------+\n",
      "|  count|               150|               150|                150|               150|               150|      150|\n",
      "|   mean|              75.5| 5.843333333333335|  3.057333333333334|3.7580000000000027| 1.199333333333334|     null|\n",
      "| stddev|43.445367992456916|0.8280661279778637|0.43586628493669793|1.7652982332594662|0.7622376689603467|     null|\n",
      "|    min|                 1|               4.3|                2.0|               1.0|               0.1|   Setosa|\n",
      "|    max|               150|               7.9|                4.4|               6.9|               2.5|Virginica|\n",
      "+-------+------------------+------------------+-------------------+------------------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_iris.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VectorAssembler \n",
    "\n",
    "A transformer that combines a given list of columns into a single vector column for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+-------+-------------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|Species|     features|\n",
      "+---+-------------+------------+-------------+------------+-------+-------------+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2| Setosa|[5.1,3.5,1.4]|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2| Setosa|[4.9,3.0,1.4]|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2| Setosa|[4.7,3.2,1.3]|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2| Setosa|[4.6,3.1,1.5]|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2| Setosa|[5.0,3.6,1.4]|\n",
      "+---+-------------+------------+-------------+------------+-------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vecAssembler = VectorAssembler(inputCols=[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\"], outputCol=\"features\")\n",
    "df_features = vecAssembler.transform(df_iris)\n",
    "df_features.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+-------+-----------------+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|Species|         features|\n",
      "+---+-------------+------------+-------------+------------+-------+-----------------+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2| Setosa|[5.1,3.5,1.4,0.2]|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2| Setosa|[4.9,3.0,1.4,0.2]|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2| Setosa|[4.7,3.2,1.3,0.2]|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2| Setosa|[4.6,3.1,1.5,0.2]|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2| Setosa|[5.0,3.6,1.4,0.2]|\n",
      "+---+-------------+------------+-------------+------------+-------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vecAssembler = VectorAssembler(inputCols=[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"], outputCol=\"features\")\n",
    "df_features = vecAssembler.transform(df_iris)\n",
    "df_features.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|   Species|\n",
      "+----------+\n",
      "| Virginica|\n",
      "|    Setosa|\n",
      "|Versicolor|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unique values in column\n",
    "df_features.select('Species').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+-------+-----------------+-----+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|Species|         features|label|\n",
      "+---+-------------+------------+-------------+------------+-------+-----------------+-----+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2| Setosa|[5.1,3.5,1.4,0.2]|  2.0|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2| Setosa|[4.9,3.0,1.4,0.2]|  2.0|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2| Setosa|[4.7,3.2,1.3,0.2]|  2.0|\n",
      "|  4|          4.6|         3.1|          1.5|         0.2| Setosa|[4.6,3.1,1.5,0.2]|  2.0|\n",
      "|  5|          5.0|         3.6|          1.4|         0.2| Setosa|[5.0,3.6,1.4,0.2]|  2.0|\n",
      "+---+-------------+------------+-------------+------------+-------+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transfer species into number\n",
    "\n",
    "# StringIndexer encodes a string column of labels to a column of label indices.\n",
    "stringIndexer = StringIndexer(inputCol=\"Species\", outputCol=\"label\")\n",
    "df_features_lable = stringIndexer.fit(df_features).transform(df_features)\n",
    "df_features_lable.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|label|\n",
      "+-----+\n",
      "|  0.0|\n",
      "|  1.0|\n",
      "|  2.0|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_features_lable.select('label').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data/ Test data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+------------+-------+-----------------+-----+\n",
      "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|Species|         features|label|\n",
      "+---+-------------+------------+-------------+------------+-------+-----------------+-----+\n",
      "|  1|          5.1|         3.5|          1.4|         0.2| Setosa|[5.1,3.5,1.4,0.2]|  2.0|\n",
      "|  2|          4.9|         3.0|          1.4|         0.2| Setosa|[4.9,3.0,1.4,0.2]|  2.0|\n",
      "|  3|          4.7|         3.2|          1.3|         0.2| Setosa|[4.7,3.2,1.3,0.2]|  2.0|\n",
      "+---+-------------+------------+-------------+------------+-------+-----------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = df_features_lable.randomSplit([.8, .2])\n",
    "df_train.count()\n",
    "\n",
    "df_train.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes model\n",
    "\n",
    "simple multiclass classification algorithm with the assumption of independence between every pair of features.\n",
    "\n",
    "class pyspark.ml.classification.NaiveBayes(self, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", \n",
    "probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\", smoothing=1.0, modelType=\"multinomial\", thresholds=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train Naive Bayes model\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label\")\n",
    "nb_model = nb.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+--------------------+--------------------+----------+\n",
      "|         features|label|       rawPrediction|         probability|prediction|\n",
      "+-----------------+-----+--------------------+--------------------+----------+\n",
      "|[5.0,3.4,1.5,0.2]|  2.0|[-14.014669680563...|[0.09449191112867...|       2.0|\n",
      "|[4.8,3.0,1.4,0.1]|  2.0|[-12.795068216499...|[0.10132642496646...|       2.0|\n",
      "|[5.7,3.8,1.7,0.3]|  2.0|[-15.825390788806...|[0.08502740174948...|       2.0|\n",
      "+-----------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "df_predicted = nb_model.transform(df_test.select('features','label'))\n",
    "df_predicted.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8148148148148148"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "evaluator.evaluate(df_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the model\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "test0 = sc.parallelize([Row(features=Vectors.dense([5.1, 3.5, 1.4, 0.2]))]).toDF()\n",
    "result = nb_model.transform(test0).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(features=DenseVector([5.1, 3.5, 1.4, 0.2]), rawPrediction=DenseVector([-14.1737, -13.419, -11.9843]), probability=DenseVector([0.0829, 0.1764, 0.7406]), prediction=2.0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export and import trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export model and import model, i.e. save and load model\n",
    "output_dir = \"hdfs:///user/aadsyanw/models/NaiveBayesModel_Iris\"\n",
    "nb_model.save(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayesModel\n",
    "model2 = NaiveBayesModel.load(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result2 = model2.transform(test0).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2.prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dtree = DecisionTreeClassifier(maxDepth=5, featuresCol=\"features\", labelCol=\"label\")\n",
    "dtree_model = dtree.fit(df_train)\n",
    "dtree_model.numNodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtree_model.depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict\n",
    "result = dtree_model.transform(test0).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(features=DenseVector([5.1, 3.5, 1.4, 0.2]), rawPrediction=DenseVector([0.0, 0.0, 40.0]), probability=DenseVector([0.0, 0.0, 1.0]), prediction=2.0)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DecisionTreeClassificationModel (uid=DecisionTreeClassifier_4792a3e28072f894f99d) of depth 5 with 15 nodes\\n  If (feature 2 <= 1.9)\\n   Predict: 2.0\\n  Else (feature 2 > 1.9)\\n   If (feature 2 <= 4.8)\\n    If (feature 3 <= 1.6)\\n     Predict: 1.0\\n    Else (feature 3 > 1.6)\\n     If (feature 0 <= 5.9)\\n      Predict: 1.0\\n     Else (feature 0 > 5.9)\\n      Predict: 0.0\\n   Else (feature 2 > 4.8)\\n    If (feature 3 <= 1.6)\\n     If (feature 2 <= 4.9)\\n      Predict: 1.0\\n     Else (feature 2 > 4.9)\\n      If (feature 0 <= 6.0)\\n       Predict: 0.0\\n      Else (feature 0 > 6.0)\\n       Predict: 0.0\\n    Else (feature 3 > 1.6)\\n     Predict: 0.0\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notes of tree\n",
    "dtree_model.toDebugString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/scala/#org.apache.spark.ml.classification.DecisionTreeClassificationModel\n",
    "\n",
    "https://spark.apache.org/docs/2.1.0/ml-features.html#vectorassembler\n",
    "\n",
    "https://spark.apache.org/docs/1.5.2/ml-features.html\n",
    "    \n",
    "https://spark.apache.org/docs/1.2.0/mllib-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
